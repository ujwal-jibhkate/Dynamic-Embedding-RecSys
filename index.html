<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From Clicks to Connections: Building a Smarter Recommender with Embeddings</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

        /* NOTION THEME - LIGHT MODE (DEFAULT) */
        :root {
            --bg-color: #f7f7f5; /* Notion's subtle off-white background */
            --content-bg: #ffffff;
            --text-primary: #37352f; /* Notion's primary text color */
            --text-secondary: rgba(55, 53, 47, 0.65);
            --accent-color: #007aff;
            --divider-color: rgba(55, 53, 47, 0.09);
            --code-bg: rgba(55, 53, 47, 0.07);
            --code-text: #eb5757;
            --callout-bg: rgba(235, 236, 237, 0.6);
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            background-color: var(--bg-color);
            color: var(--text-primary);
            margin: 0;
            padding: 4em 1em;
            display: flex;
            justify-content: center;
            transition: background-color 0.2s ease;
        }

        .container {
            max-width: 720px;
            width: 100%;
            background-color: var(--content-bg);
            padding: 0 2em; /* Padding is now horizontal only */
        }
        
        @media (min-width: 768px) {
            .container {
                padding: 0 3em;
            }
        }

        header {
            text-align: left;
            margin-bottom: 3em;
        }

        h1 {
            font-size: 2.5em;
            font-weight: 700;
            letter-spacing: -0.02em;
            margin: 0 0 0.2em 0;
        }

        h2 {
            font-size: 1.7em;
            font-weight: 600;
            letter-spacing: -0.01em;
            margin-top: 2.5em;
            margin-bottom: 0.8em;
        }
        
        h3 {
            font-size: 1.3em;
            font-weight: 600;
            margin-top: 1.5em;
        }

        p, ul {
            font-size: 1.05em;
            color: var(--text-primary);
            margin-bottom: 1.2em;
        }

        strong, b {
            font-weight: 600;
        }

        blockquote {
            margin: 1.5em 0;
            padding-left: 1em;
            border-left: 3px solid var(--text-primary);
            font-size: 1.05em;
            color: var(--text-secondary);
        }
        
        hr {
            border: 0;
            height: 1px;
            background-color: var(--divider-color);
            margin: 3em 0;
        }

        code {
            background-color: var(--code-bg);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-family: "SF Mono", "Fira Code", "Fira Mono", "Roboto Mono", monospace;
            font-size: 0.9em;
            color: var(--code-text);
        }

        a {
            color: var(--accent-color);
            text-decoration: none;
            font-weight: 500;
            border-bottom: 1px solid rgba(0, 122, 255, 0.2);
        }

        a:hover {
            border-bottom-color: var(--accent-color);
        }
        
        .conclusion {
            background-color: var(--callout-bg);
            border: none;
            padding: 1.5em;
            border-radius: 6px;
            margin-top: 2em;
        }
        
        .conclusion h3 {
            margin-top: 0;
        }

        .embedded-content {
            margin-top: 2.5em;
        }

        .embedded-content img, .embedded-content iframe {
            width: 100%;
            border-radius: 6px;
            border: 1px solid var(--divider-color);
            margin-top: 1.5em;
        }

        .embedded-content iframe {
            height: 550px;
        }

        .caption {
            text-align: center;
            font-style: normal;
            font-size: 0.9em;
            color: var(--text-secondary);
            margin-top: 0.5em;
        }

        /* NOTION THEME - DARK MODE ðŸŒ’ */
        @media (prefers-color-scheme: dark) {
            :root {
                --bg-color: #191919;
                --content-bg: #191919;
                --text-primary: rgba(255, 255, 255, 0.9);
                --text-secondary: rgba(255, 255, 255, 0.5);
                --accent-color: #0a84ff;
                --divider-color: rgba(255, 255, 255, 0.1);
                --code-bg: rgba(255, 255, 255, 0.1);
                --code-text: #ff808a;
                --callout-bg: rgba(255, 255, 255, 0.05);
            }
            .embedded-content img {
                background-color: #ffffff;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>From Clicks to Connections: Building a Smarter Recommender with Embeddings</h1>
        </header>

        <article>
            <p>It all started in an Applied Machine Learning course, where my professor discussed his research into linguistic embeddingsâ€”representing complex features of language as simple lists of numbers, or <strong>vectors</strong>. I was captivated. How could numbers capture so much meaning? While the exact "how" is still a puzzle being solved by researchers in fields like <strong>mechanistic interpretability</strong>, their power is undeniable.</p>

            <blockquote>
                <b>So, what exactly is an embedding? Letâ€™s break it down.</b><br>
                Imagine a giant, detailed map. Instead of cities, this map plots the "location" of concepts. On this map, concepts with similar meanings are placed close together. For example, the words 'king' and 'queen' would be neighbors, just as 'walking' and 'running' would be. An <b>embedding</b> is simply the "GPS coordinate" of a concept on this mapâ€”a list of numbers that tells us exactly where it sits.<br><br>
                These maps are learned by neural networks. By analyzing massive amounts of data (like all of Wikipedia), a network learns the relationships between concepts. The embedding is the networkâ€™s internal, numerical summary of a concept, learned from the context in which it appears.
            </blockquote>
            
            <p>My fascination deepened when I learned about <strong>superposition</strong>, a concept allowing embeddings to store an incredible amount of information in high-dimensional space. It struck me that this was a powerful tool for representing <em>anything</em>. This got me thinking about the classic recommendation challenges, like the <strong>"cold start" problem</strong> faced by services like Netflix. How do you recommend something to a brand-new user? Traditional methods often struggle here. That's when two ideas clicked: What if I could use the power of embeddings to build a better system?</p>
            
            <p>My hypothesis was to create a fixed embedding for every movie and a dynamic, evolving embedding for each user. To solve the cold start issue, the system would initialize a user's profile by asking for 3-5 movies they already love. This creates an initial taste profile that gets refined with every new movie they interact with. To put this theory to the test, I built a proof-of-concept by curating a custom dataset from a large movie database and training a few small feedforward Neural Networks to bring the idea to life.</p>
            
            <p>This article documents that journey, phase by phase.</p>

            <hr>

            <h2>Phase 1: Assembling the Multimodal Movie Dataset</h2>
            <p>A robust recommendation system is built on rich, high-quality data. My primary goal was to create a truly <strong>multimodal embedding</strong> for each filmâ€”one that captures more than just text. This required assembling a comprehensive dataset that describes each movie from multiple angles: its plot, its creative personnel, and its visual tone.</p>
            <p>The process began by merging two core datasets from Kaggle's "The Movies Dataset," followed by extensive <strong>feature engineering</strong> to parse valuable data locked in text files. Key roles like the director, writers, and main actors were extracted to form the textual backbone of our dataset. To introduce multimodality, I incorporated visual features from the movie posters, processing over 5,000 posters to extract their average RGB colorâ€”a simple proxy for visual tone.</p>
            <p>With the primary dataset of over 44,000 movies assembled, a final data preparation step was necessary. An <strong>Exploratory Data Analysis (EDA)</strong> revealed a significant genre imbalance. To create a smaller, manageable dataset for rapid prototyping without this bias, we employed <strong>stratified sampling</strong>. This technique ensured our final sample of 2,000 movies maintained the exact same proportional representation of each genre as the original dataset, making it a high-fidelity miniature perfect for reliable model training.</p>
            
            <hr>

            <h2>Phase 2: Learning Similarity with a Multi-Loss Fusion Model</h2>
            <p>The heart of this project lies not just in using pre-trained embeddings, but in teaching a custom model to <strong>fuse</strong> them in a way that truly understands movie similarity. The goal was to create a single "digital DNA" for each film that simultaneously encodes its genre, creative style, and key talent.</p>
            <p>First, I established a foundation by generating two separate vectors for each movie: a <strong>Visual Embedding</strong> from the poster using a CLIP model, and a <strong>Textual Embedding</strong> from all written content using a Sentence-BERT model. With these two vectors, the challenge was to train a fusion network to combine them intelligently.</p>
            <p>To capture the complexity of what makes movies "similar," I implemented a sophisticated <strong>multi-loss training strategy</strong>. I defined similarity in three different ways: shared genres, shared directors, and shared actors. During each step of training, the model calculated three separate <strong>Triplet Loss</strong> values, one for each similarity type. These three losses were then merged into a single, combined loss signal. By forcing the model to minimize this combined loss, it learned to create a rich, unified embedding space that respects all three conditions at once.</p>

            <section class="embedded-content">
                <h3>Visualization 1: The Learned Embedding Space</h3>
                <p>This interactive visualization plots the final fused embeddings for our 2,000-movie dataset. Notice how movies with similar genres, directors, or actors cluster together, demonstrating the success of our multi-loss training strategy.</p>
                <iframe src="visualizations/movie_embeddings_visualization.html" title="Movie Embedding Space Visualization"></iframe>
            </section>

            <hr>
            
            <h2>Phase 3: Dynamic User Embeddings - A Learning Taste Profile</h2>
            <p>With a "digital DNA" for every movie, the next challenge was to create one for each <strong>user</strong>. This user embedding needed to be a living representation of their unique taste, capable of evolving as they interact with new films.</p>
            <p>Our system bypasses the "cold start" problem with a <strong>"warm start"</strong> approach. A new user's profile is initialized by averaging the embeddings of 3-5 movies they already love. From there, a custom neural network called the <strong><code>UserUpdateFFN</code></strong> intelligently refines the user's embedding after each new movie interaction. A key feature of this model is a <strong>residual connection</strong>, which means the network only learns to predict the <em>change</em> or <em>delta</em> to the user's profile. This leads to much more stable learning and ensures a user's taste profile evolves smoothly without forgetting their core preferences.</p>
            <p>We trained this updater model by simulating thousands of user interactions, again using Triplet Loss. The model was taught a simple but profound rule: "When a user likes a movie, update their profile so it moves significantly closer to the movie they liked, and simultaneously moves away from movies they are unlikely to care about."</p>
            
            <section class="embedded-content">
                <h3>Visualization 2: Evolving User Profile</h3>
                <p>This simulation shows how a user's taste profile evolves. As you "like" new movies from the recommendations, watch how your profile vector moves through the embedding space, getting closer to your preferences.</p>
                <iframe src="visualizations/user_embeddings_update_visualization.html" title="Dynamic User Profile Visualization"></iframe>
            </section>

            <hr>

            <h2>Phase 4: The Recommendation Pipeline - Retrieval and Ranking</h2>
            <p>A production-grade system requires a sophisticated, two-step pipeline to be both fast and intelligent: <strong>Candidate Retrieval</strong> and <strong>Ranking</strong>.</p>
            <p>To learn the affinity between users and movies, I implemented a <strong><code>Two-Tower Model</code></strong>, with one tower for users and one for items. This model was trained to predict whether a user would like a movie, learning to project users and movies into a shared space where proximity indicates a good match.</p>
            <p>For the retrieval step, we used <strong>FAISS (Facebook AI Similarity Search)</strong>. We indexed all our specialized movie vectors into FAISS, which allows us to instantly retrieve the top 100 most relevant candidates for any user without a slow, exhaustive search. For the ranking step, we used <strong>Maximal Marginal Relevance (MMR)</strong> to refine this list of 100 candidates. MMR iteratively builds the final recommendation list by selecting items that strike the best balance between <strong>relevance</strong> to the user and <strong>diversity</strong> from items already selected, ensuring the final list is both accurate and interesting.</p>
            
            <section class="embedded-content">
                <h3>System Architecture</h3>
                <p>The diagram below illustrates the complete two-stage recommendation pipeline, from initial user input to the final, reranked list of movies.</p>
                <img src="visualizations/Embedding_Project_Phases.png" alt="System architecture diagram showing the two-stage retrieval and ranking pipeline.">
                <p class="caption">The complete recommendation pipeline.</p>
            </section>

            <hr>

            <h2>Phase 5: Evaluation - Measuring Success and Finding Balance</h2>
            <p>The final phase was to quantitatively measure our system's performance. Our evaluation was framed around two equally important, and often competing, goals: <strong>Relevance</strong> (accuracy) and <strong>Diversity</strong> (variety).</p>
            <ul>
                <li><strong>For Relevance</strong>, we used a suite of metrics including Precision@k, Recall@k, and our primary metric, <strong>nDCG@k (Normalized Discounted Cumulative Gain)</strong>, which rewards placing the best recommendations at the top of the list.</li>
                <li><strong>For Diversity</strong>, we used <strong>Intra-List Similarity (ILS)</strong>, where a lower score indicates a more varied and interesting list.</li>
            </ul>
            <p>Through experimentation, we tuned the lambda (Î») parameter in our MMR reranker, which controls the balance between these two goals. We identified <strong>lambda = 0.5</strong> as the optimal "sweet spot," as it achieved near-peak relevance scores while providing a significant boost in diversity.</p>
            
            <div class="conclusion">
                <h3>ðŸ“– Conclusion: A Prototype with Production Potential</h3>
                <p>This project successfully demonstrates a powerful, end-to-end framework for building a modern recommendation system. The final evaluation metrics show that the system provides highly relevant recommendations while maintaining healthy diversity. More importantly, the architectureâ€”using a two-stage pipeline, FAISS for scalability, and modular, updatable modelsâ€”was designed with real-world application in mind, showing strong potential to evolve from a prototype into a production-ready system.</p>
            </div>
            
        </article>
    </div>
</body>
</html>