{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount the drives\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwwUN73mlKoM",
        "outputId": "8c00e596-2402-4fea-9463-ed4aefcba453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-riDb-imvx3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration and Loading ---\n",
        "print(\"✅ 1. Loading All Models and Data for Final Evaluation...\")\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/Embedding_Based_Recommendations_Project/Datasets/final_datasets/'\n",
        "USER_EMBEDDINGS_PATH = os.path.join(DRIVE_BASE_PATH, 'final_user_embeddings.parquet')\n",
        "MOVIE_EMBEDDINGS_PATH = os.path.join(DRIVE_BASE_PATH, 'movie_content_embeddings_multitask.parquet')\n",
        "USER_INTERACTIONS_PATH = os.path.join(DRIVE_BASE_PATH, 'user_movie_interactions.parquet')\n",
        "TWO_TOWER_MODEL_PATH = os.path.join(DRIVE_BASE_PATH, 'two_tower_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VuPuxoIkU09",
        "outputId": "02084c8d-e137-47fb-f491-8e2520a1fc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 1. Loading All Models and Data for Final Evaluation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Model Architecture to Load Weights ---\n",
        "LATENT_DIM = 128\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, user_dim=512, item_dim=512, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.user_tower = nn.Sequential(nn.Linear(user_dim, latent_dim * 2), nn.ReLU(), nn.Linear(latent_dim * 2, latent_dim))\n",
        "        self.item_tower = nn.Sequential(nn.Linear(item_dim, latent_dim * 2), nn.ReLU(), nn.Linear(latent_dim * 2, latent_dim))\n",
        "    def forward(self, user_vecs, item_vecs):\n",
        "        user_latent = nn.functional.normalize(self.user_tower(user_vecs), p=2, dim=1)\n",
        "        item_latent = nn.functional.normalize(self.item_tower(item_vecs), p=2, dim=1)\n",
        "        return user_latent, item_latent\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "two_tower_model = TwoTowerModel()\n",
        "two_tower_model.load_state_dict(torch.load(TWO_TOWER_MODEL_PATH))\n",
        "two_tower_model.to(device)\n",
        "two_tower_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX40yvpjkbQ7",
        "outputId": "40da1035-ad84-4b98-e76d-7792b5e71b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TwoTowerModel(\n",
              "  (user_tower): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  )\n",
              "  (item_tower): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data ---\n",
        "user_embs_df = pd.read_parquet(USER_EMBEDDINGS_PATH)\n",
        "movies_df = pd.read_parquet(MOVIE_EMBEDDINGS_PATH)\n",
        "interactions_df = pd.read_parquet(USER_INTERACTIONS_PATH)\n",
        "movie_embeddings = np.array(movies_df['content_embedding'].tolist())\n",
        "\n",
        "# --- Prepare Data Structures ---\n",
        "user_embs_tensor = torch.tensor(user_embs_df.values, dtype=torch.float32).to(device)\n",
        "movie_embs_tensor = torch.tensor(movie_embeddings, dtype=torch.float32).to(device)\n",
        "user_id_to_idx = {uid: i for i, uid in enumerate(user_embs_df.index)}\n",
        "movie_id_to_idx = {mid: i for i, mid in enumerate(movies_df['tmdb_id'])}\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, all_movie_latents = two_tower_model(torch.zeros(len(movies_df), 512).to(device), movie_embs_tensor)\n",
        "all_movie_latents_np = all_movie_latents.cpu().numpy()\n",
        "\n",
        "faiss_index = faiss.IndexFlatIP(LATENT_DIM)\n",
        "faiss_index.add(all_movie_latents_np)\n",
        "print(\"Data and models loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAd_O-edkeBD",
        "outputId": "7d7e5cf1-185b-4752-b93d-f18085c9941e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data and models loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Create Train/Test Split ---\n",
        "print(\"\\n✅ 2. Creating Train/Test Split for Evaluation...\")\n",
        "train_interactions = {}\n",
        "test_interactions = {}\n",
        "\n",
        "for _, row in interactions_df.iterrows():\n",
        "    user_id = row['userId']\n",
        "    watched = row['watched_movie_ids']\n",
        "    if len(watched) >= 5: # Need at least 5 interactions to create a meaningful split\n",
        "        train_interactions[user_id] = set(watched[:-2]) # Use all but the last 2 for history\n",
        "        test_interactions[user_id] = set(watched[-2:])  # Hold out the last 2 as the ground truth\n",
        "\n",
        "print(f\"Split data for {len(test_interactions)} users.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMyusHYrkhJP",
        "outputId": "a18faca7-2e3b-47ae-d313-f456187a1548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 2. Creating Train/Test Split for Evaluation...\n",
            "Split data for 269 users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Metric Calculation Functions ---\n",
        "def precision_recall_at_k(recommendations, ground_truth, k):\n",
        "    rec_set = set(recommendations[:k])\n",
        "    truth_set = set(ground_truth)\n",
        "    hits = len(rec_set.intersection(truth_set))\n",
        "\n",
        "    precision = hits / k\n",
        "    recall = hits / len(truth_set) if len(truth_set) > 0 else 0\n",
        "    return precision, recall\n",
        "\n",
        "def ndcg_at_k(recommendations, ground_truth, k):\n",
        "    rec_set = recommendations[:k]\n",
        "    dcg = 0\n",
        "    for i, item_id in enumerate(rec_set):\n",
        "        if item_id in ground_truth:\n",
        "            dcg += 1 / np.log2(i + 2) # i+2 because ranks are 1-based, log is 2-based\n",
        "\n",
        "    # Ideal DCG: assumes all ground truth items are at the top\n",
        "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(ground_truth), k)))\n",
        "    return dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "def intra_list_diversity(recommendations, movie_latents_map):\n",
        "    rec_latents = [movie_latents_map[rec_id] for rec_id in recommendations if rec_id in movie_latents_map]\n",
        "    if len(rec_latents) < 2: return 0.0\n",
        "\n",
        "    # Calculate cosine similarity between all pairs of items in the list\n",
        "    similarity_matrix = cosine_similarity(rec_latents)\n",
        "    # Diversity is 1 - average similarity (upper triangle of matrix, excluding diagonal)\n",
        "    upper_triangle_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
        "    avg_similarity = np.mean(similarity_matrix[upper_triangle_indices])\n",
        "    return 1 - avg_similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "CW1CGNgIklHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Main Evaluation Loop ---\n",
        "print(\"\\n✅ 3. Running Main Evaluation Loop...\")\n",
        "K = 10\n",
        "all_precisions, all_recalls, all_ndcgs, all_ilds = [], [], [], []\n",
        "recommended_item_pool = set()\n",
        "movie_id_to_latent = {mid: all_movie_latents_np[i] for mid, i in movie_id_to_idx.items()}\n",
        "\n",
        "# Re-using the MMR function from the previous script\n",
        "def rerank_with_mmr(user_vec, candidate_indices, candidate_latents, top_k=10, lambda_param=0.7):\n",
        "    final_recs_indices = []\n",
        "    relevance_scores = cosine_similarity(user_vec, candidate_latents)[0]\n",
        "    while len(final_recs_indices) < top_k and len(candidate_indices) > 0:\n",
        "        best_score, best_idx_pos = -np.inf, -1\n",
        "        for i in range(len(candidate_indices)):\n",
        "            diversity_score = 0\n",
        "            if final_recs_indices:\n",
        "                selected_latents = all_movie_latents_np[final_recs_indices]\n",
        "                dissimilarity = 1 - cosine_similarity(candidate_latents[i].reshape(1, -1), selected_latents)\n",
        "                diversity_score = np.min(dissimilarity)\n",
        "            mmr_score = lambda_param * relevance_scores[i] + (1 - lambda_param) * diversity_score\n",
        "            if mmr_score > best_score:\n",
        "                best_score, best_idx_pos = mmr_score, i\n",
        "        best_item_original_idx = candidate_indices.pop(best_idx_pos)\n",
        "        final_recs_indices.append(best_item_original_idx)\n",
        "        relevance_scores = np.delete(relevance_scores, best_idx_pos)\n",
        "        candidate_latents = np.delete(candidate_latents, best_idx_pos, axis=0)\n",
        "    return final_recs_indices\n",
        "\n",
        "\n",
        "for user_id, ground_truth_ids in tqdm(test_interactions.items(), desc=\"Evaluating\"):\n",
        "    # --- Generate recommendations for the user ---\n",
        "    user_idx = user_id_to_idx[user_id]\n",
        "    user_vec = user_embs_tensor[user_idx].unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        user_latent, _ = two_tower_model(user_vec, torch.zeros(1, 512).to(device))\n",
        "    user_latent_np = user_latent.cpu().numpy()\n",
        "\n",
        "    _, candidate_indices = faiss_index.search(user_latent_np, 100)\n",
        "    candidate_indices = list(candidate_indices[0])\n",
        "    candidate_latents = all_movie_latents_np[candidate_indices]\n",
        "\n",
        "    final_rec_indices = rerank_with_mmr(user_latent_np, candidate_indices, candidate_latents, top_k=K)\n",
        "    final_rec_ids = [movies_df['tmdb_id'].iloc[i] for i in final_rec_indices]\n",
        "\n",
        "    # --- Calculate Metrics ---\n",
        "    precision, recall = precision_recall_at_k(final_rec_ids, ground_truth_ids, K)\n",
        "    ndcg = ndcg_at_k(final_rec_ids, ground_truth_ids, K)\n",
        "    ild = intra_list_diversity(final_rec_ids, movie_id_to_latent)\n",
        "\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_ndcgs.append(ndcg)\n",
        "    all_ilds.append(ild)\n",
        "    recommended_item_pool.update(final_rec_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "569352eccb3d46ada64aa9981a343025",
            "8655e50999d1434aa6750dc5369ce80f",
            "fd7ab516a8c14fcbb65724a649c44dc1",
            "ca71dc5d0cbf41c1bb5b00b9e14d639d",
            "b0ef031624c542ab9971dff1a19f62d2",
            "727b6904af9c42cfa31038d1e92b69ea",
            "3299f4a4da58432ca8da2e5fd58afd4a",
            "ce580c3fb00a4973ab945f4065fd8116",
            "049189c7a06f4059a49e7b20f71b3ec8",
            "04eaf34d309f440286f7dedf66290ea0",
            "c321df4b6b644d77906d472b0737a5b9"
          ]
        },
        "id": "bGmkVpfHko79",
        "outputId": "7704e8dc-b3ca-4a25-e90c-ad23346d0f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 3. Running Main Evaluation Loop...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/269 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "569352eccb3d46ada64aa9981a343025"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMX7p1GgkIg-",
        "outputId": "fabf296d-d318-480d-9302-2e0142c71eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "      FINAL EVALUATION METRICS\n",
            "==================================================\n",
            "Precision@10:      0.0230\n",
            "Recall@10:         0.1152\n",
            "NDCG@10:           0.0674\n",
            "--------------------------------------------------\n",
            "Intra-List Div.@10: 0.0268\n",
            "Catalog Coverage:   1.77%\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Display Final Results ---\n",
        "avg_precision = np.mean(all_precisions)\n",
        "avg_recall = np.mean(all_recalls)\n",
        "avg_ndcg = np.mean(all_ndcgs)\n",
        "avg_ild = np.mean(all_ilds)\n",
        "catalog_coverage = len(recommended_item_pool) / len(movies_df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"      FINAL EVALUATION METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Precision@{K}:      {avg_precision:.4f}\")\n",
        "print(f\"Recall@{K}:         {avg_recall:.4f}\")\n",
        "print(f\"NDCG@{K}:           {avg_ndcg:.4f}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Intra-List Div.@{K}: {avg_ild:.4f}\")\n",
        "print(f\"Catalog Coverage:   {catalog_coverage:.2%}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Main Evaluation Loop with Lambda Tuning ---\n",
        "print(\"\\n✅ 3. Running Main Evaluation Loop with Lambda Tuning...\")\n",
        "\n",
        "K = 10\n",
        "lambda_values_to_test = [0.7, 0.5, 0.3] # 0.7 (Relevance), 0.5 (Balanced), 0.3 (Diversity)\n",
        "final_results = []\n",
        "\n",
        "# (The MMR re-ranking function from the previous script goes here)\n",
        "def rerank_with_mmr(user_vec, candidate_indices, candidate_latents, top_k=10, lambda_param=0.7):\n",
        "    final_recs_indices = []\n",
        "    relevance_scores = cosine_similarity(user_vec, candidate_latents)[0]\n",
        "    candidate_indices = list(candidate_indices) # Make a mutable copy\n",
        "\n",
        "    while len(final_recs_indices) < top_k and len(candidate_indices) > 0:\n",
        "        best_score, best_idx_pos = -np.inf, -1\n",
        "        for i in range(len(candidate_indices)):\n",
        "            diversity_score = 0.0\n",
        "            if final_recs_indices:\n",
        "                selected_latents = all_movie_latents_np[final_recs_indices]\n",
        "                dissimilarity = 1 - cosine_similarity(candidate_latents[i].reshape(1, -1), selected_latents)\n",
        "                diversity_score = np.min(dissimilarity)\n",
        "            mmr_score = lambda_param * relevance_scores[i] + (1 - lambda_param) * diversity_score\n",
        "            if mmr_score > best_score:\n",
        "                best_score, best_idx_pos = mmr_score, i\n",
        "\n",
        "        best_item_original_idx = candidate_indices.pop(best_idx_pos)\n",
        "        final_recs_indices.append(best_item_original_idx)\n",
        "        np.delete(relevance_scores, best_idx_pos)\n",
        "        np.delete(candidate_latents, best_idx_pos, axis=0)\n",
        "\n",
        "    return final_recs_indices\n",
        "\n",
        "\n",
        "for lambda_val in lambda_values_to_test:\n",
        "    print(f\"\\n--- Evaluating with lambda = {lambda_val} ---\")\n",
        "    all_precisions, all_recalls, all_ndcgs, all_ilds = [], [], [], []\n",
        "    recommended_item_pool = set()\n",
        "    movie_id_to_latent = {mid: all_movie_latents_np[i] for mid, i in movie_id_to_idx.items()}\n",
        "\n",
        "    for user_id, ground_truth_ids in tqdm(test_interactions.items(), desc=f\"Lambda {lambda_val}\"):\n",
        "        user_idx = user_id_to_idx[user_id]\n",
        "        user_vec = user_embs_tensor[user_idx].unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            user_latent, _ = two_tower_model(user_vec, torch.zeros(1, 512).to(device))\n",
        "        user_latent_np = user_latent.cpu().numpy()\n",
        "\n",
        "        _, candidate_indices = faiss_index.search(user_latent_np, 100)\n",
        "        candidate_latents = all_movie_latents_np[candidate_indices[0]]\n",
        "\n",
        "        final_rec_indices = rerank_with_mmr(user_latent_np, candidate_indices[0], candidate_latents, top_k=K, lambda_param=lambda_val)\n",
        "        final_rec_ids = [movies_df['tmdb_id'].iloc[i] for i in final_rec_indices]\n",
        "\n",
        "        precision, recall = precision_recall_at_k(final_rec_ids, ground_truth_ids, K)\n",
        "        ndcg = ndcg_at_k(final_rec_ids, ground_truth_ids, K)\n",
        "        ild = intra_list_diversity(final_rec_ids, movie_id_to_latent)\n",
        "\n",
        "        all_precisions.append(precision)\n",
        "        all_recalls.append(recall)\n",
        "        all_ndcgs.append(ndcg)\n",
        "        all_ilds.append(ild)\n",
        "        recommended_item_pool.update(final_rec_ids)\n",
        "\n",
        "    results = {\n",
        "        \"lambda\": lambda_val,\n",
        "        \"Precision@10\": np.mean(all_precisions),\n",
        "        \"Recall@10\": np.mean(all_recalls),\n",
        "        \"NDCG@10\": np.mean(all_ndcgs),\n",
        "        \"ILD@10\": np.mean(all_ilds),\n",
        "        \"Coverage\": len(recommended_item_pool) / len(movies_df)\n",
        "    }\n",
        "    final_results.append(results)\n",
        "\n",
        "# --- 5. Display Final Results ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"              FINAL EVALUATION METRICS COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "results_df = pd.DataFrame(final_results)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "b1adf9967c9a45a89f6cc47b982bf860",
            "573903d0df244f979112d6af5b0e8502",
            "c598cdc9410746a791e607733b8c9321",
            "4e98938118294918b2e6f8dd6ef8a158",
            "dcf941ca14b64d1982e1f195d7df1dda",
            "e8a78f888c6049f4a60ccd089288d964",
            "98ab728793a849699307b224a4ca461c",
            "f77575d1f04146f98303b49ec12f1457",
            "5f01557a8e5c4521b1bda99d97e66e25",
            "736a5bf48bdc40229e5b291e783638e1",
            "38377cabae2a4be1b0ebdb44c297a4a1",
            "9447607505c14482bb5427ce65dda41d",
            "1a1872c6c291407bb11ddc9f49ac1919",
            "979b6f3294a44c3e9dd2ccb3e6f317cd",
            "24971164f9764f2cad52d797cd2df76a",
            "35d208ab38764cadbb51bd0682a66f46",
            "3537dab3479044b19f30102be0354885",
            "1e2a612956f04f5198e894c4a5d89357",
            "5fd36242058a41fabde6f99e6d656313",
            "5e32f104531545d182a6cc7f92658897",
            "8b44d403a02448de88220f5f951ec97c",
            "cd38d83b6a8b46f388f3760de52e8b57",
            "e816eb5c3ec8458c80e7be7ce7b1aa9a",
            "c109651043874862a63fb6daa0aa9e4e",
            "eee471def7c8445b9f9204e236370189",
            "a2b08a75881c440fbd6e49f6b158fb50",
            "7eff4681fa5d4613ac878fd4fa677797",
            "5878ec9ff7d940438f3b4846a04e81b2",
            "86cbbb06393b45db9e6c414ce9480c88",
            "d4effcb6ce294ba0b460a3dee717935d",
            "fe346514f9f545d38835eb93a2c7c079",
            "4e862064e24b4db08883fa21c10337e1",
            "8856f82e03ee408cb4589b4b0db34960"
          ]
        },
        "id": "NCrWTClUmZJF",
        "outputId": "f3a5eba9-b03c-47c9-ecc1-e6c34a3e1727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 3. Running Main Evaluation Loop with Lambda Tuning...\n",
            "\n",
            "--- Evaluating with lambda = 0.7 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Lambda 0.7:   0%|          | 0/269 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1adf9967c9a45a89f6cc47b982bf860"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating with lambda = 0.5 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Lambda 0.5:   0%|          | 0/269 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9447607505c14482bb5427ce65dda41d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating with lambda = 0.3 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Lambda 0.3:   0%|          | 0/269 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e816eb5c3ec8458c80e7be7ce7b1aa9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "              FINAL EVALUATION METRICS COMPARISON\n",
            "======================================================================\n",
            " lambda  Precision@10  Recall@10  NDCG@10   ILD@10  Coverage\n",
            "    0.7      0.022305   0.111524 0.066351 0.026146  0.021684\n",
            "    0.5      0.014126   0.070632 0.047615 0.038716  0.044881\n",
            "    0.3      0.013755   0.068773 0.047049 0.039266  0.030257\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}